{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "* deep learning is more suited for image recognition, computer vision, natural language processing and understanding problems\n",
    "* learning rate too high caused overfit and too small would have no impact\n",
    "* must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network and any optional metrics we would like to colelct and report during training\n",
    "* binary_corssentropy is the log loss \n",
    "* Adam: gradient descent algorithm \n",
    "* Early stopping allow trying for 3 times before it moves on to the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical \n",
    "#SGD Stochastic gradient decent optimizer\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "input_shape=len(all_features)\n",
    "print(input_shape)\n",
    "\n",
    "# This creates a two-layer sequential NN model\n",
    "def get_new_model(input_dim = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim = input_shape, activation='relu'))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)\n",
    "\n",
    "#lr_to_test=[.00001, .0001,0.01, 1]\n",
    "#lr_to_test=[0.009, 0.02, 0.03,0.04,0.05]\n",
    "# compare model with low, medium and high learning rate\n",
    "lr_to_test=[.01]\n",
    "for lr in lr_to_test:\n",
    "        print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "        model = get_new_model()\n",
    "        my_optimizer = SGD(lr=lr)\n",
    "        model.compile(optimizer=my_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        early_stopping_monitor = EarlyStopping(patience=3)\n",
    "        model.fit(X, Y, epochs=50, validation_split=0.3, batch_size=100, callbacks=[early_stopping_monitor], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function converts/transforms the input into a format that works better in the gradient descent process for machine learning:\n",
    "\n",
    "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigmoid function\n",
    "The sigmoid function converts the input in the range of (0,1). Use it for the output layer when performing binary forecast. \n",
    "\n",
    "**Formula**: a=sigmoid(z)=1/(1+e(-z))\n",
    "\n",
    "softmax function is a more generalized logistic activation function which is used for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh\n",
    "\n",
    "tanh function is a shifted sigmoid function, returns values in the range of (-1, 1)\n",
    "it works better than sigmoid function by centering the data around 0 rather than 0.5\n",
    "\n",
    "**Formula**:\n",
    "a = tanh(z) = e(z) - e(-z)/(e(z) + e(-z))\n",
    "\n",
    "<img src=\"images/sigmoid vs tanh.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n",
    "\n",
    "Your NN learns much faster with ReLU than with sigmoid/tanh. \n",
    "\n",
    "**Formula**: a=max(0,z)\n",
    "    \n",
    "<img src=\"images/sigmoid vs relu.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Cheat Sheet\n",
    "\n",
    "<img src=\"images/activation cheat sheet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for NN\n",
    "\n",
    "<img src=\"images/gradient descent for nn.jpg\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35622124 -0.90113397 -0.42475039]\n",
      " [-0.00492433  0.01713343  1.1135313 ]\n",
      " [ 3.1632273   0.00556174  0.58684141]\n",
      " [-0.7251957  -0.72182714 -0.10614139]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.randn(4,3)\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.96966312]\n",
      " [ 1.1257404 ]\n",
      " [ 3.75563044]\n",
      " [-1.55316423]]\n"
     ]
    }
   ],
   "source": [
    "B = np.sum(A, axis=1, keepdims=True)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL, \n",
    "                                                                                    current_cache[1]), \n",
    "                                                                                       current_cache[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
